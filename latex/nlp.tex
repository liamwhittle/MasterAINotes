\documentclass[]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}
\usepackage{listings}

\title{NLP - Master Notes}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1} % for allowing us to do do while loops

\begin{document}
	\maketitle
	
	\section{Introduction}
	
	\section{Automata and Regex}
	
	\section{Language Models}
	
	\subsection{N-Gram Models}
	
	\begin{itemize}
	
		\item Language (prediction) models which make the \textit{Markov assumption} for an $(n-1)^{th}$ order Markov Model; i.e. that only the previous n-1 words have a probabilistic dependence on the current word.
		
		\item Probability of words 1 to n: $P(w_1^n) = \prod_{k=1}^{n}P(w_k | w_{k-N+1}^{k-1})$
		
		General steps for creating an n-gram model:		
		\begin{enumerate}
			\item choose a vocabulary 
			\item add $<s>$ and $</s$ symbols 
			\item replace unknown words in the training corpus with $<UNK>$ 
			\item calculate probabilities (on an as needs basis?)
			\item calculate most probably words in order (until reaching end of sentence symbol) OR evaluate perplexity of test corpus using above formulas.
			
		\end{enumerate}
		
		\item \textit{example:} 
		
		\begin{center}
			Text: \\
			One cat sat. Three \textbf{cats sat}. Eight \textbf{cats sat}. The \textbf{cats} had nine lives. 	\\
			
			P(sat $|$ cats sat) = $\frac{C(cats,sat)}{C(cat)} = \frac{2}{3}$
			
		\end{center}
		
		\item \textbf{Definition of a language model}: a model which assigns probabilities to sentences, based on the training corpus. The sum of all the probabilities of all possible sentences (of arbitrary length) should equal 1. \\ 
			
			Trivial example:
			\begin{itemize}
				\item Training corpus contains two sentences: \\ 
				1. ``a b", \\ 
				2. ``b a"
				
				\item Append $<sos>$ and $</sos>$ to each: \\
				1. ``s a b /s" 
				\\ 2. ``s b a /s"
				
				\item generate the probabilities of a bigram (N=2) language model: \\
				
				$P(a | s) = \frac{1}{2}$ \\
				$P(b | s) = \frac{1}{2}$ \\
				$P(b | a) = \frac{1}{2}$ \\
				$P(a | b) = \frac{1}{2}$ \\
				$P(/s | a) = \frac{1}{2}$ \\
				$P(/s | b) = \frac{1}{2}$ \\
				
				\item To calculate the probability of ALL possible sentences, we take the prob of all sentences of length 1, all sentences of length 2, etc. Note when calculating this, the TEST sentences need to include $<s> and </s>$
				
				\item For example: P(a) = P($<s> a </s>$) = $P(a | <s>) * P(</s> | a)$ = 1/4
				
				\item Probability is same for b, so the sum of all probabilities of sentence length 1 = $\frac{1}{2}$
				
				The sum of all sentences will be the infinite series: \\
				
				$$P(all) = \frac{1}{2} + \sum_{i=2}^{\infty} \frac{i!}{(i-2)!} \frac{1}{2} ^ {i + 1}$$ \\
				
				Does this sum to 1? A proof would be cool.
				
			\end{itemize}
		
		\item Sentence Generation: until you produce a $</s>$ symbol, continually generate words using: $argmax_(w_k) \frac{C(w_{k-n+1},...,w_k)}{C(w_{k-n+1},...,w_{k-1})}$ 
		
		\item Perplexity: how well a model fits the data $PP(W) = P(w1,...w_N)^{-\frac{1}{N}}$, for N words in the test corpus. A perplexity of 1 would be the lowest possible.
		
		\item Smoothing: 
		
			\begin{itemize}
				\item Laplace (add-one): $P(W_n | w_{n - N + 1}^{n-1}) = \frac{C( w_{n - N + 1}^{n-1}w) + 1}{c( w_{n - N + 1}^{n-1}) + V + 1}$, where V is the vocabulary size
				
				\item add $\delta$, normalise by $\delta V$
			
			\end{itemize}
		
	\item Interpolation: creating a linear combination of n-gram models of varying n: $\hat{P}(w_i | w_{i-1},...w_{i-n}) = \sum_{j=2}^{n} \lambda_j * P(w_i | w_{i-1},...,w_{i-1-j})$ , where $\sum_{j}\lambda_j = 1$ 
	
	\item Back-off: back-off to lower n models until data is available (does this mean you can have arbitrary n?)
	
	\end{itemize}
	
	
	
	\clearpage
	\section{Part of Speech Tagging}
	
	A Part of speech tagger assigns for every word $w_i$ in a sentence $\{w_i, ..., w_n\}$ a part of speech (POS) tag $y_i$. POS tags include parts of speech like verb (tag=V), noun, adverb etc.
	
	\textbf{Naive Baseline}: More than half of words are ambiguous - i.e. they have more than one possible tag. If we count all the tags in a training set and produce $P(tag | word)$ for each word, we can simply choose the most likely tag for the word. This baseline has an accuracy of about 92\%, only 5\% less than the state of the art. 
	
	All extra efforts in more complex models (e.g. CRF, HMM) are about squeezing out this last 5\%. 
	
	Named entity tagging (e.g. Washington State) is harder - it assumes groups of words together refer to one particular proper noun. 
	
	\subsection{Hidden Markov Models}
	Suppose there are T tags (hidden states). An HMM will require a transition matrix giving the probability of any tag occurring after any other tag. This will be an T x T matrix of probabilities (we can compute this similarly to the bigram model, using the bigram assumption) called the \textbf{transition matrix}
	\\
	Suppose also there are N words. The HMM will also require probabilities of each word given a tab. This will be a T * N matrix. Again, this is computed using counts of word-tag pairs over total tag counts, called the \textbf{emission matrix}
	\\
	``The goal of part-of-speech tagging is to find the most probably sequence of $t_1...t_n$ tags given a sequence of N words $w_1...w_n$."  
	\\
	We achieve the goal of the HMM by the following equations:
	
	\begin{align}	
		t_{1:n} = argmax_{t_1...t_n} P(t_1...t_n | w_1...w_n) &\tag*{max prb tags given words} \\	
		t_{1:n} = argmax_{t_1...t_n} \frac{P(w_1...w_n | t_1...t_n)}{P(w_1...w_n)} &\tag*{by bayes rule} \\	
		t_{1:n} = argmax_{t_1...t_n} P(w_1...w_n | t_1...t_n) &\tag*{since argmax, can discard denom} \\	
		t_{1:n} = argmax_{t_1...t_n} \prod_{i=1}^n P(w_i|t_i) \prod_{i=1}^n P(t_i|t_{i-1}) &\tag*{by the HMM assumptions} \\	
	\end{align}
	
	Note: we also require an array of initial probabilities for each of the states. \\
	
	We are interested in a tag sequence satisfying the above equation. Brute forcing would be possible, but would take $O(T^N)$ (infeasible). 
	
	The viterbi algorithm attempts to solve this problem in a feasible time.
	
	\textit{The viterbi algorithm takes in the following inputs:} \\ O: observation space, S: state space, $\pi$: initial probabilities, Y:sequence of observations, A: transition matrix, B: emission matrix. 
	
	
	\begin{algorithm}
		\caption{Viterbi}
		\begin{algorithmic}[1]
			\Procedure{Viterbi}{O, S, $\pi$, Y, A, B}  
            
            % \Comment{There are K possible states, T observations}
            
            \State $P \gets initMatix(rowSize=K, colSize=K)$ \Comment{for max probabilities}
            \State $M \gets initMatix(rowSize=K, colSize=K)$ \Comment{for argmax of above}
            
            
            \For{state i =1,2,...,K} \Comment{Assign the first set of probabilities}
                \State $P[i, 1] = \pi_i * B_{iy_1}$
                \State $M[i, 1] = 0$
            \EndFor
            
            \\ \\
             \Comment{Prob = max of previous max states leading to this 
            state and evidence}
            \For{observation j = 2,3,...,K}
                
                \For{state i = 1,2,...K}
                
                    \State $P[i, j] = max_k(P[k, j-1] * A_{ki} * B_{iy_i})$
                    \State $M[i, j] = argmax_k(P[k, j-1] * A_{ki} * B_{iy_i})$
                
                \EndFor
            
            \EndFor
           
            \\ \\
            
            \Comment{FinalState = max of final probs} 
            \State $z_T = argmax_k (P[k, T])$
            \\ \\
            \Comment{Backtrace through back pointers to construct output}
            \\
            \State $X \gets initArray(size=T)$
            \State $X[T] = z_T$
            \For{j = T, T-1, ..., 2}
            
                \State $X[j-1] = S_{M[X[J], j]}$
            
            \EndFor
            
            \State $return(X)$
            
            \EndProcedure
            
		\end{algorithmic}
	\end{algorithm}
	
	\subsection{Unsupervised HMM Learning for POS Tagging} 
	Given \textit{only} a known number of states and an un-labelled corpus (a series of observations), we wish to estimate the parameters of a hidden markov model which maximise the probability of seeing those states. 
	\\
	We can use maximum likelyhood principle to perform expectation maximisation.
	\\
	We can also perform semi-supervised learning to exploit small amounts of labelled training data.
	
	\clearpage
	\section{Statistical Parsing}
	
	\subsection{Overview}
	
	\subsection{Parse Trees, CNF, CFG, PCFG}
	
	\textbf{Parse Trees}: show the groupings of words in a sentence to their syntactic group. E.g. Noun Phrase (NP), Verb Phrase (VP). This helps downstream NLP tasks to determine the meaning of sentences, perform translations, etc. \\
	
	\textbf{Converting to Chomsky Normal Form:}
	Chomsky Normal Form means that the right hand side of each rule must expand to two non terminals, or one terminal. \cite{jurafsky2018speech}
	
	\begin{enumerate}
		\item Copy all conforming rules to the new grammar unchanged
		\item Convert terminals within rules to dummy non-terminals
		\item Convert unit productions
		\item Make all rules binary and add them to new grammar.
	\end{enumerate}
	

	\textbf{PCFG}: each rule is assigned a probability, and the sum of all probabilities per non terminal (on the left hand side of the rule) = 1.

	
	\subsection{CKY Algorithm, Probabilistic CKY}
	
	CYK is a \textbf{bottom up} parser - i.e. we start with the words and build the tree to the top. 
	
	CKY algorithm requires a grammar to be normalised to Chomsky Normal Form. This will naturally result in an (likely unbalanced) binary tree when expanded. \\
	
	The \textbf{time complexity} of CKY parsing is $O(n^3)$, since there are $n^2$ cells to fill, and each cell requires querying n split points (\textit{not 100\% sure about this point}). 
	
    \clearpage
	\section{Statistical Machine Translation}
	
	\textbf{Linguistic issues with machine translation:}
	
	\begin{itemize}
	 \item \textbf{Lexical divergence} - not all words in one language have a word in the other
	 \item \textbf{Syntactical divergence} (e.g. SVO vs SOV)
	 \item \textbf{Morphology} - languages with polysynthetic structures make it harder to represent the language as sets of words, because a single word can represent an entire sentence.   
	\end{itemize}
	
	
	
	\textbf{Vauqouis Triangle}:
	
	\begin{itemize}
		\item Words 
		\item parsing -  syntactic structure
		\item SRL \& WSD - Semantic Structure
		\item Semantic Parsing -  Interlingua
	\end{itemize}
	
	\clearpage
	\section{Neural Machine Translation}
	
	The key difference between Neural and Statistical Machine Translation (NMT, SMT) is that SMT treats the problem of translation as a word to word alignment problem, where the alignment is a latent variable. NMT treats the entire source to target sentence as a more complicated function. \\
	
	\textbf{Key areas:}
	\begin{itemize}
	 \item Perceptrons are good for learning at the character level, good for linearly seperable data, however cannot solve certain problems (e.g. XOR)
	 \item Feed forward neural networks can be used for learning characters using CNNs (e.g. MNIST)
	 \item Sequence to Sequence (seq-2-seq) models are conditional language models
	 \item LSTMs and GRUs are recurrent units which can perform sequence to sequence translation with memory. Hidden state of LSTM, for instance, can show embedded context of phrases.
	 \item Attention: these models learn which components of each sequence are relevant to each other. Higher layers learn semantics, lower layers are better and morphology etc.
	 \item ELMO: creating context specific embeddings for weighted task specific embeddings to be fed into the system. 
	 \item Transformers: Attention is All You Need: similar to CNN but no recursion. Embeddings are positional. 
	 \item BERT: deep bidirectional transformer
	 \item ELMO and BERT can also be used for word sense dissambiguation, coreference resolution.
	 \item BERT: good for some things, but cannot reason about it's knowledge (it cannot draw logical inferences from sentences). Struggles representing numbers, Representations are hierarchical rather than linear.
	\end{itemize}

	
	\clearpage 
	\section{Semantic Parsing}
	
	\subsection{Meaning Representation}
	Question: how to do question answering of natural language (\textbf{NL})? \\
	Entailment for question answering: $$FOL \gets parse\_to\_FOL(NL) $$ \\
              $$ answer \models FOL $$ \\
              $$ answer \gets parse\_to\_english(answer) $$ \\
              
    \textit{Model Theoretic Semantics} - defines the meaning of formal logic byp providing the set theoretic extensions of symbols (e.g. an interpretation of their extension to the real word/ model).
	
	
	\subsection{Semantic Role Labelling (SRL) Concepts}
	
	E.g. nominative and accusative. Some possible roles include: \textit{Agent, Patient, Instrument, Beneficiary, Source, Destination} (roles aren't limited to people, they can be assigned to places, things etc.)
	
	There \textit{can} be syntactic clues (e.g. preposition words - ``with", ``for", ``from" etc.), however ultimately very difficult to do.
	
	\textbf{Selectional Restrictions}: by assigning a role to a concept, we imply certain characteristics of the concept. E.g.: a thing which is an "agent" should be animate (and not inanimate). Taxonomic abstraction hierarchies can be used to determine if such constraints are met (e.g. human = mammal = vertebrate = animate). 
	
	\subsection{SRL Approaches}
	
	1. \textit{Idea:} first apply a syntactic parse tree, then label the concepts \\
	SRL $\subset$ \textit{Sequence Labelling} \\
	Inputs = Parse Tree Nodes \\
	Classifier: labels the parse tree nodes with roles \\
	
	\textit{Features for classifier: }
	\begin{itemize}
	 \item Paths from the candidate node to be labelled to the predicate along the parse tree.
     \item position: does the candidate preceed or follow the predicate?
     \item voice: active or passive?
	\end{itemize}
    
    Issues: relies on the correctness of syntactic parsing, which often has errors. Parse Tree (errors) --- Classifier based SRL (more issues) \\ 
    
	Semantic roles can be indicated by syntactic features/ locations of words in sentence, but this is not generally useful as there are many exceptions. \\
	
	\textit{Selectional Restrictions:} Some roles place restrictions on other roles: e.g. the patient of 'eat' should be a type of food.
	
	\subsection{Predicate Calculus / Functional Query Language}
	
	% sentence -> predicate logic
	\begin{align}
	\text{Sentence} \implies \text{Predicate Logic}
	\end{align}

	\textbf{Sentence:} What is the smallest state by area? \\
	\textbf{Predicate Logic:} answer($x_1$, smallest($x_2$,(state($x_1$), area($x_1$,$x_2$)))) \\
	
	% predicate logic -> functiona query language
	\begin{align}
		\text{Predicate Logic} \implies \text{Functional Query Language (variable free)}
	\end{align}
	
	\textbf{FOL:} answer($x_1$, smallest($x_2$,(state($x_1$), area($x_1$,$x_2$)))) \\
	\textbf{FQL:} answer(smallest\_one(area\_1(state(all)))) \\
	
	\subsection{Composing Meaning Representations from Parse Trees}
	\textit{Need to review this material - second half of lecture 8}
	
	\clearpage
	\section{Lexical Semantics}
	
	\subsection{Word Sense Dissambiguation}
	
	\textit{Word Sense Disambiguation:} which meaning is the word referring to? \\
	
	\textit{WordNet} is a lexical database linking words to nodes (meanings) with various relationships, e.g.: antonyms, attributes, similarness, causes, entailment, hyponym (plant is a hyponym of tree as it is a more specific instance). \\
	
	\textbf{How to achieve word sense dissambiguation:} 
	
	\begin{enumerate}
	 \item Conduct POS tagging
	 \item Get list of possible senses of (word, POS) from WordNet
	 \item Encode the context of the word
	 \item Train a classifier on this data (input = word, POS, contect, output = word sense).
	 \item does this mean we need seperate classifiers per word?
	\end{enumerate}
    
    \textbf{Feature Encoding For Word Context:} \\
	
	\begin{itemize}
	 \item unordered surrounding bag of word (in sparse vector form) - gives general topic
	 \item POS of neighbouring words
	 \item ``Local Collocations'' - immediate neighbours of a word ot give context
	\end{itemize}

	\subsection{The Yarowsky Algorithm}
	Semi-supervised method which first trains on the supervised data, then repeatedly expands the set of ``supervised'' training data by labelling unsupervised data for which the classifier is extremely confident. Then, the classifier re-trains with the larger supervised set until it is complete. At each iteration, increase the number of features (``seed words'').
	
	\subsection{Coreference Resolution}
	The task of dissambiguating which pronouns refer to the same entity. E.g. \textbf{I} took \textbf{my} cat to the vet. Here, I and my refer to the same thing.
	
	Steps: 1. detect the mentions of things (look for pronouns). 2. Cluster the mentions (challenging!). \\
	
	Difficult recognition cases: ``\textbf{It} is sunny''. Here, ``it'' is a reference we should care about, but it's not a pro-noun, propper noun etc. Should we filter out singleton mentions to make clulstering easier? \\
	
	Could also do reference detection and coreference resolution end to end.\\

	\textbf{Anaphora}: specifically when pronouns refer to previously refered concepts
	
	\subsection{Hobb's naive algorithm}
	
	\clearpage
	\section{Vector Space Semantics}
    
    \subsection{Word Weighting}
    
    \begin{itemize}
     \item Weights: term which appear more frequently  are more important. should normalize term frequency by dividing by the most common term in the document.
     \item Inverse document frequency: $idf_i = log_2(\frac{N}{df_i})$, where N is the number of documents, $df_i$ is the number of documents containing term i. Log is for smoothing. Higher values for more unusual terms. $idf_i \in [0, \infty]$. E.g. when a word i is in literally every document, $idf_i$ = 0. If a term is in every 16 documents, $idf_i = 4$
     \item Typical tf-idf weighting: $w_{ij} = tf_{ij}idf_j$ for word i in document j.
     \item Cosine Similarity: $$cossim(\hat{a}, \hat{b}) = \frac{\hat{a} \cdot \hat{b}}{|\hat{a}| \cdot |\hat{b}|}$$
     
     $$cossim(\hat{a}, \hat{b}) 
     = \frac
     {\sum_i^{len(\hat{a})} \hat{a_i} * \hat{b_i}}
     {\sqrt{\sum_i^{len(\hat{a})}\hat{a_i} * \sum_i^{len(b)}\hat{b_i}}}$$
     
    \end{itemize}

    
    
      \textbf{Distributed Representations}: Bag of words approach above makes extremely large sparse matrices. Need a more compact way of representing...
     
     PCA or SVD can achieve this. Basically: which dimensions capture the most variance/ information within the data? \\
     
     \subsection{Word-2-Vec:}
     
     The simplest and most naive way to represent words from a dictionary as a vector would be with one-hot encoding (sparce binary matrices with dictionary size). This is a poor use of memory for single words or ordered words, but reasonable if we want to represent dense un-ordered sets of words. What it doesn't do however is capture any useful information about the words and how they are related. Isn't \textbf{orange} similar to \textbf{apple} more than it is similar to \textbf{exuberantly}? \\
     
     It turns out by using a variation of a \textit{neural autoencoder} to predict \textit{context words} we can create vector representations of words which are both \textbf{more compact } (smaller dimension) and \textbf{geometrically useful}. By the later point, I mean to say \textbf{apple} and \textbf{orange} will have a smaller (some metric) distance than apple and \textbf{exuberantly}.
     
     The two variations of this auto-encoder, \textit{Skip-Gram} and \textit{CBOW}, both produce the same thing, but use different approaches to training.
     
     The assumption which both of these methods use is that similarity of words can be learned by taking immediate context words as relevant information. This does mean we loose the ability to consider long term dependencies, however word-2-vec is more of a corpus wide average than a question answering system, so it should be a fair assumption to make. \\
     
     \textbf{SkipGram}
     \begin{itemize}
      \item Input: word to be embedded
      \item Output: surrounding context words
      \item Good for: Higher dimensions, large corpus
     \end{itemize}

     
     \textbf{CBOW}
     \begin{itemize}
      \item Input: surrounding context words
      \item Output: word to be embedded
      \item Good for: smaller corpus, faster training
     \end{itemize}
    
    \subsection{Keyword Extraction}
    
   
    \clearpage
	\section{Discourse}
	Linguistic Devices: Cue phrases, paragraphs, content flow. \\
	Discourse relations: ways in which sentences relate to each other. \\
	Types: 
	\begin{itemize}
	 \item Explanation: Provides explanation for previous sentence. (what was the cause from the effect)
	 \item Elaboration: Adding more information to a previous sentence. 
	 \item Result: A thing happened as a result of the previous sentence (what is the effect of the cause).
	 \item Parallel: Extra information  
	 \item Occasion: What was the situational motivation for doing something?
	\end{itemize}
	
	We can construct a binary tree using nodes as these discourse types, and leafes as sentences and other nodes. We could also constructa a graph, a flat structure etc. Multiple theories of discourse. \\
	
	\textbf{Rhetorical Structure Theory (RST)} \\
	Considers nucleus's and satellites. The nucleus is more important.
	Elementary Discourse Units (EDU) form satelites. Nucleus can be a group of related satelites / nucleus's. \\ 
	
	\textbf{PDTB} - flat structure, WSJ, larger database

	
	
	\clearpage
	% bibliography
	\bibliographystyle{ieeetran}
	\bibliography{nlp_references}
	
\end{document}
