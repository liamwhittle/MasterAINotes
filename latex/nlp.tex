\documentclass[]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}
\usepackage{listings}

\title{NLP - Master Notes}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1} % for allowing us to do do while loops

\begin{document}
	\maketitle
	
	\section{Language Models}
	
	\subsection{N-Gram Models}
	
	\begin{itemize}
	
		\item Language (prediction) models which make the \textit{Markov assumption} for an $(n-1)^{th}$ order Markov Model; i.e. that only the previous n-1 words have a probabilistic dependence on the current word.
		
		\item Probability of words 1 to n: $P(w_1^n) = \prod_{k=1}^{n}P(w_k | w_{k-N+1}^{k-1})$
		
		General steps for creating an n-gram model:		
		\begin{enumerate}
			\item choose a vocabulary 
			\item add $<s>$ and $</s$ symbols 
			\item replace unknown words in the training corpus with $<UNK>$ 
			\item calculate probabilities (on an as needs basis?)
			\item calculate most probably words in order (until reaching end of sentence symbol) OR evaluate perplexity of test corpus using above formulas.
			
		\end{enumerate}
		
		\item \textit{example:} 
		
		\begin{center}
			Text: \\
			One cat sat. Three \textbf{cats sat}. Eight \textbf{cats sat}. The \textbf{cats} had nine lives. 	\\
			
			P(sat $|$ cats sat) = $\frac{C(cats,sat)}{C(cat)} = \frac{2}{3}$
			
		\end{center}
		
		\item \textbf{Definition of a language model}: a model which assigns probabilities to sentences, based on the training corpus. The sum of all the probabilities of all possible sentences (of arbitrary length) should equal 1. \\ 
			
			Trivial example:
			\begin{itemize}
				\item Training corpus contains two sentences: \\ 
				1. ``a b", \\ 
				2. ``b a"
				
				\item Append $<sos>$ and $</sos>$ to each: \\
				1. ``s a b /s" 
				\\ 2. ``s b a /s"
				
				\item generate the probabilities of a bigram (N=2) language model: \\
				
				$P(a | s) = \frac{1}{2}$ \\
				$P(b | s) = \frac{1}{2}$ \\
				$P(b | a) = \frac{1}{2}$ \\
				$P(a | b) = \frac{1}{2}$ \\
				$P(/s | a) = \frac{1}{2}$ \\
				$P(/s | b) = \frac{1}{2}$ \\
				
				\item To calculate the probability of ALL possible sentences, we take the prob of all sentences of length 1, all sentences of length 2, etc. Note when calculating this, the TEST sentences need to include $<s> and </s>$
				
				\item For example: P(a) = P($<s> a </s>$) = $P(a | <s>) * P(</s> | a)$ = 1/4
				
				\item Probability is same for b, so the sum of all probabilities of sentence length 1 = $\frac{1}{2}$
				
				The sum of all sentences will be the infinite series: \\
				
				$$P(all) = \frac{1}{2} + \sum_{i=2}^{\infty} \frac{i!}{(i-2)!} \frac{1}{2} ^ {i + 1}$$ \\
				
				Does this sum to 1? A proof would be cool.
				
			\end{itemize}
		
		\item Sentence Generation: until you produce a $</s>$ symbol, continually generate words using: $argmax_(w_k) \frac{C(w_{k-n+1},...,w_k)}{C(w_{k-n+1},...,w_{k-1})}$ 
		
		\item Perplexity: how well a model fits the data $PP(W) = P(w1,...w_N)^{-\frac{1}{N}}$, for N words in the test corpus. A perplexity of 1 would be the lowest possible.
		
		\item Smoothing: 
		
			\begin{itemize}
				\item Laplace (add-one): $P(W_n | w_{n - N + 1}^{n-1}) = \frac{C( w_{n - N + 1}^{n-1}w) + 1}{c( w_{n - N + 1}^{n-1}) + V}$, where V is the vocabulary size
				
				\item add $\delta$, normalise by $\delta V$
			
			\end{itemize}
		
	\item Interpolation: creating a linear combination of n-gram models of varying n: $\hat{P}(w_i | w_{i-1},...w_{i-n}) = \sum_{j=2}^{n} \lambda_j * P(w_i | w_{i-1},...,w_{i-1-j})$ , where $\sum_{j}\lambda_j = 1$ 
	
	\item Back-off: back-off to lower n models until data is available (does this mean you can have arbitrary n?)
	
	\end{itemize}
	
	
	
	\clearpage
	\section{Part of Speech Tagging}
	
	A Part of speech tagger assigns for every word $w_i$ in a sentence $\{w_i, ..., w_n\}$ a part of speech (POS) tag $y_i$. POS tags include parts of speech like verb (tag=V), noun, adverb etc.
	
	\textbf{Naive Baseline}: More than half of words are ambiguous - i.e. they have more than one possible tag. If we count all the tags in a training set and produce $P(tag | word)$ for each word, we can simply choose the most likely tag for the word. This baseline has an accuracy of about 92\%, only 5\% less than the state of the art. 
	
	All extra efforts in more complex models (e.g. CRF, HMM) are about squeezing out this last 5\%. 
	
	Named entity tagging (e.g. Washington State) is harder - it assumes groups of words together refer to one particular proper noun. 
	
	\subsection{Hidden Markov Models}
	Suppose there are T tags (hidden states). An HMM will require a transition matrix giving the probability of any tag occurring after any other tag. This will be an T x T matrix of probabilities (we can compute this similarly to the bigram model, using the bigram assumption) called the \textbf{transition matrix}
	\\
	Suppose also there are N words. The HMM will also require probabilities of each word given a tab. This will be a T * N matrix. Again, this is computed using counts of word-tag pairs over total tag counts, called the \textbf{emission matrix}
	\\
	``The goal of part-of-speech tagging is to find the most probably sequence of $t_1...t_n$ tags given a sequence of N words $w_1...w_n$."  
	\\
	We achieve the goal of the HMM by the following equations:
	
	\begin{align}	
		t_{1:n} = argmax_{t_1...t_n} P(t_1...t_n | w_1...w_n) &\tag*{max prb tags given words} \\	
		t_{1:n} = argmax_{t_1...t_n} \frac{P(w_1...w_n | t_1...t_n)}{P(w_1...w_n)} &\tag*{by bayes rule} \\	
		t_{1:n} = argmax_{t_1...t_n} P(w_1...w_n | t_1...t_n) &\tag*{since argmax, can discard denom} \\	
		t_{1:n} = argmax_{t_1...t_n} \prod_{i=1}^n P(w_i|t_i) \prod_{i=1}^n P(t_i|t_{i-1}) &\tag*{by the HMM assumptions} \\	
	\end{align}
	
	Note: we also require an array of initial probabilities for each of the states. \\
	
	We are interested in a tag sequence satisfying the above equation. Brute forcing would be possible, but would take $O(T^N)$ (infeasible). 
	
	The viterbi algorithm attempts to solve this problem in a feasible time.
	
	\textit{The viterbi algorithm takes in the following inputs:} \\ O: observation space, S: state space, $\pi$: initial probabilities, Y:sequence of observations, A: transition matrix, B: emission matrix. 
	
	
	\begin{algorithm}
		\caption{Viterbi}
		\begin{algorithmic}[1]
			\Procedure{Viterbi}{O, S, $\pi$, Y, A, B}  
            
            % \Comment{There are K possible states, T observations}
            
            \State $P \gets initMatix(rowSize=K, colSize=K)$ \Comment{for max probabilities}
            \State $M \gets initMatix(rowSize=K, colSize=K)$ \Comment{for argmax of above}
            
            
            \For{state i =1,2,...,K} \Comment{Assign the first set of probabilities}
                \State $P[i, 1] = \pi_i * B_{iy_1}$
                \State $M[i, 1] = 0$
            \EndFor
            
            \\ \\
             \Comment{Prob = max of previous max states leading to this 
            state and evidence}
            \For{observation j = 2,3,...,K}
                
                \For{state i = 1,2,...K}
                
                    \State $P[i, j] = max_k(P[k, j-1] * A_{ki} * B_{iy_i})$
                    \State $M[i, j] = argmax_k(P[k, j-1] * A_{ki} * B_{iy_i})$
                
                \EndFor
            
            \EndFor
           
            \\ \\
            
            \Comment{FinalState = max of final probs} 
            \State $z_T = argmax_k (P[k, T])$
            \\ \\
            \Comment{Backtrace through back pointers to construct output}
            \\
            \State $X \gets initArray(size=T)$
            \State $X[T] = z_T$
            \For{j = T, T-1, ..., 2}
            
                \State $X[j-1] = S_{M[X[J], j]}$
            
            \EndFor
            
            \State $return(X)$
            
            \EndProcedure
            
		\end{algorithmic}
	\end{algorithm}
	
	\subsection{Unsupervised HMM Learning for POS Tagging} 
	Given \textit{only} a known number of states and an un-labelled corpus (a series of observations), we wish to estimate the parameters of a hidden markov model which maximise the probability of seeing those states. 
	\\
	We can use maximum likelyhood principle to perform expectation maximisation.
	\\
	We can also perform semi-supervised learning to exploit small amounts of labelled training data.
	
	\clearpage
	\section{Statistical Parsing}
	
	\subsection{Overview}
	
	\subsection{Parse Trees, CNF, CFG, PCFG}
	
	\textbf{Parse Trees}: show the groupings of words in a sentence to their syntactic group. E.g. Noun Phrase (NP), Verb Phrase (VP). This helps downstream NLP tasks to determine the meaning of sentences, perform translations, etc. \\
	
	\textbf{Converting to Chomsky Normal Form:}
	Chomsky Normal Form means that the right hand side of each rule must expand to two non terminals, or one terminal. \cite{jurafsky2018speech}
	
	\begin{enumerate}
		\item Copy all conforming rules to the new grammar unchanged
		\item Convert terminals within rules to dummy non-terminals
		\item Convert unit productions
		\item Make all rules binary and add them to new grammar.
	\end{enumerate}
	

	\textbf{PCFG}: each rule is assigned a probability, and the sum of all probabilities per non terminal (on the left hand side of the rule) = 1.

	
	\subsection{CKY Algorithm, Probabilistic CKY}
	
	CYK is a \textbf{bottom up} parser - i.e. we start with the words and build the tree to the top. 
	
	CKY algorithm requires a grammar to be normalised to Chomsky Normal Form. This will naturally result in an (likely unbalanced) binary tree when expanded. \\
	
	The \textbf{time complexity} of CKY parsing is $O(n^3)$, since there are $n^2$ cells to fill, and each cell requires querying n split points (\textit{not 100\% sure about this point}). 
	
	
	\section{Statistical Machine Translation}
	
	\textbf{Linguistic issues with machine translation:} \textit{Lexical divergence (not all words in one language have a word in the other), syntactical divergence (e.g. SVO vs SOV), morphology} \\
	
	\textbf{Vauqouis Triangle}:
	
	\begin{itemize}
		\item Words 
		\item parsing -  syntactic structure
		\item SRL \& WSD - Semantic Structure
		\item Semantic Parsing -  Interlingua
	\end{itemize}
	
	
	\section{Semantic Parsing}
	
	\subsection{Semantic Role Labelling (SRL) - concepts}
	
	E.g. nominative and accusative. Some possible roles include: \textit{Agent, Patient, Instrument, Beneficiary, Source, Destination} (roles aren't limited to people, they can be assigned to places, things etc.)
	
	There \textit{can} be syntactic clues (e.g. preposition words - "with", "for", "from" etc.), however ultimately very difficult to do.
	
	\textbf{Selectional Restrictions}: by assigning a role to a concept, we imply certain characteristics of the concept. E.g.: a thing which is an "agent" should be animate (and not inanimate). Taxonomic abstraction hierarchies can be used to determine if such constraints are met (e.g. human = mammal = vertebrate = animate). 
	
	\subsection{SRL Approaches}
	
	\textit{Idea:} first apply a syntactic parse tree, then label the concepts \\
	SRL $\subset$ \textit{Sequence Labelling} \\
	Labels = Parse Tree Nodes \\
	
	Can use a variety of supervised machine learning methods.
	
	\subsection{Predicate Calculus / Functional Query Language}
	
	% sentence -> predicate logic
	\begin{align}
	\text{Sentence} \implies \text{Predicate Logic}
	\end{align}

	\textbf{Sentence:} What is the smallest state by area? \\
	\textbf{Predicate Logic:} answer($x_1$, smallest($x_2$,(state($x_1$), area($x_1$,$x_2$)))) \\
	
	% predicate logic -> functiona query language
	\begin{align}
		\text{Predicate Logic} \implies \text{Functional Query Language (variable free)}
	\end{align}
	
	\textbf{FOL:} answer($x_1$, smallest($x_2$,(state($x_1$), area($x_1$,$x_2$)))) \\
	\textbf{FQL:} answer(smallest\_one(area\_1(state(all)))) \\
	
	
	% bibliography
	\bibliographystyle{ieeetran}
	\bibliography{nlp_references}
	
\end{document}
