\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}

\title{ML - Master Notes}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}

\begin{document}
	\maketitle
	
	\section{Elements of ML}
	
	\subsection{Key Terms}
	
	\begin{itemize}
		 
		\item inference, prediction, classification, regression
		\item training set, test set, generalisation
		\item supervised learning, unsupervised learning
		\item clustering
		\item parametric model, non-parametric model
		\item mathematical optimization
		\item sum of squared errors: $E(w) = \frac{1}{2}\sum{[y(x_n, w) - t_n]^2}$ | $w* = argmin_w E(w)$
		\item closed form solutions/ analytic solutions, iterative algorithms
		\item over fitting, under fitting
		\item root mean squared error: $E_{RMS} := \sqrt{2E(w*)/N}$ $\implies$ compares different data set sizes equally
		\item Regularization: accounting for the desire to not over-fit by including a penalty term in the error function. For example:
			$E_R(w) = E(w) + \frac{\lambda}{2}\sum_{i=0}^{n}{w_i^2}$, where $\lambda$ is a regularization parameter governing the importance of the model complexity penalty
		
		\item validation set, k-fold cross validation
		\item leave one out cross validation: k-fold cross validation where k = n, the number of data points
		\item K-nearest neighbours
		\item uncertainty, frequentist, bayesian
		\item maximum likelihood principle
		\item bootstrap: use MLE to predict parameters for a model to fit multiple alternative data sets,
		and measure the variance of the parameters over the distribution of data sets
		For example, to bootstrap we may select a data sets from D by taking N data points with replacement
		
		\subsection{Regularisation}
		The key idea here is to achieve the perfect balance between bias (over-fitting) and variance (under-fitting). There are many ways of achieving this. For instance:
		
		\begin{itemize}
			\item setting \textbf{M}, the number of hidden units, can limit the complexity of the function which can be modelled.
			\item \textbf{Weight Decay}: start with a large value of \textbf{M} and add a regularisation parameter to the error function.
			\item \textbf{Early Stopping}: stop at the point of lowest error for the validation set, not the train set.
		\end{itemize} 
		
	\end{itemize}
	
	
	\clearpage
	\section{Regression}
	
	\textit{Definition of regression:} to predict the continuous target value of one or more variables
	
	\subsection{Linear Regression}
	
	\begin{itemize}
	\item Linear regression models only need to be linear with respect to the adjustable parameters.
	\item They do not need to be linear with respect to the input variables.
	\item Therefore, polynomial regression is also linear regression (e.g. $f(x) = \alpha x^2$) but multi-layer neural networks are not.
	
	\item For more advanced linear regression, we can take a linear combination of a number of non linear functions of the input variables.
	
	\item Example of a linear model:
	$F(X, w_1, w_2) = w_1 * \sin{x} + w_2 * x^2$
	
	\item Example of a non linear model:
	$F(X, w_1) = w_1^2 * x$
	
	\item The non-linear functions we use on the input variables are called basis functions
	
	\item Single layer neural networks which sum the weighted inputs are linear models!
	
	\end{itemize}
	
	\subsection{Error function}
	Choice of \textit{error function} is linked to maximum likelihood principle. \\
	
	Let us do some analysis where the key assumption is that noise is gaussian:
	
	Let our training set be the set of ${(x_n, t_n)}$ pairs where x is an input
	vector and t is the output value.
	
	We assume there is a true deterministic function giving rise to t:
	$t = t(x, w) + \epsilon$, where $\epsilon$ is gaussian noise with zero mean and
	variance $\sigma^2$
    
    So we can write \\
    
    $$p(t|\mu, w, \sigma^2) = N(t|y(x, w), \sigma^2)$$    
    
    $$p(t_1,...,t_N|x_1,...x_N) = \prod_{n=1}^N N(t_n | y(x_n, w), \sigma^2)$$ \\
    
    Now we can define the log likelyhood as:
    
    $$L(\theta) = log \prod_{n=1}^N N(t_n | y(x_n, w), \sigma^2)$$
    
    $$ = \sum_{n=1}^N 
    log \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}\frac{(x - \mu)}{\sigma}^2}$$
       
       
    $$ =  N \log \frac{1}{\sqrt{2\pi}} + \sum_{n=1}^N \log e^{-\frac{1}{2}\frac{(x - \mu)}{\sigma}^2}$$
    
    $$ =  N \log \frac{1}{\sqrt{2\pi}} + \sum_{n=1}^N {-\frac{1}{2}\frac{(x - \mu)}{\sigma}^2}$$
    
	\subsection{Gradient Descent}
	
	The gradient descent algorithm is described below, given a training set and basis functions $\phi$ \\
	
	First we need to define the cost function, $J(\theta)$:
	
	$$J(\theta) = \frac{1}{N} \sum_{n=1}^N \frac{1}{2} {||h_{\theta}(x^n) - y^n||}^2 + \frac{\lambda}{2}\sum_{l=1}^{n_l-1}\sum_{l=1}^{s_l}\sum_{j=1}^{s_l+1}{W_{ji}^{s_l+1}}^2$$
    
	\begin{enumerate}
	
	\item Initialize random weights $\pmb{w}$, set regularization parameter $\lambda$, stopping condition $\epsilon$, and training rate $\eta$
	\item While the difference between weights from the previous step is less than $\epsilon$:
	
	\setlength{\itemindent}{2em}
	\item Randomly shuffle the training data rows
	\item For each training example input and output pair $(\pmb{x}_n, y_n)$:
	
	\setlength{\itemindent}{4em}
	\item Calculate the gradient vector using \newline  
	$\frac{d\pmb{e}_{L2}}{d\pmb{w}} = -(y_n - \pmb{w} * \phi(\pmb{x_n}))\phi(\pmb{x_n}) + \lambda\pmb{w}$
	
	\item Perform weight update: $\pmb{w} = \pmb{w} - \eta \frac{d\pmb{e}_{L2}}{d\pmb{w}}$
    
   
	\end{enumerate}
	
	\clearpage
	\section{Classification} \cite{elements}
	
	In classification, you can either model a decision boundary itself (e.g. a set of hyper-planes) - this would be called a \textbf{discriminative model}, or a probabilistic generative model (e.g. a bayesian classifier) which models the probability that 
	
	\subsection{Discriminative Models}
	\textit{Definition}: Non probabilistic models which directly assign any input to one class.
	
	\textbf{Perceptron} \\
	The perceptron for \textit{two classes} is a generalised linear model with a step function deciding which class to apply to, i.e.:
	
	$\hat{y} = f(\mathbf{w} * \phi(x) + b)$, where f is 1 for values greater than 0, else -1. \\
	Since it is a non differentiable function, we must use the ``perceptron update" function instead of back prop. 
	
	\subsection{Probabilistic Generative Models}
	\textit{Definition}: Infer the class posterior from the class prior and the class conditional, which we derive from the data (e.g. Naive Bayes). \\
	 
	
	\textbf{Benefits} of generative models: by modelling the underlying distributions, we can generate synthetic data (e.g. by plugging the generated parameters into a probability model and using RNG).
	
	\textbf{Detriments} of generative models: often they have more parameters than discriminative models. \\ 
	
	\textit{Mathematically}: $P(C_k | x) = P(C_k) * P(x | C_k)$ \\ 
	
	
	
	\subsection{Probabilistic Discriminative Models}
	\textit{Definition}: Probabilistic model which assigns a probability per class input pair.
	
	
	\clearpage
	\section{Latent Variable Models}
	
	\subsection{K-Means Clustering}
	
	\begin{itemize}
		\item K-means is sensitive to the initial clusters - no guarantee it will converge to the same clusters each time
		\item K means is a form of clustering, which is a form of unsupervised learning
	\end{itemize}
	
	\begin{algorithm}
		\caption{K-Means Clustering}\label{alg:euclid}
		\begin{algorithmic}[1]
			\Procedure{K-Means}{$\mathbf{X},K$}\Comment{$\mathbf{X}$ are points $\{x_n\}, n \in 1..N}, K clusters$
			\State $C \gets initClusters(K)$ \Comment{C is a set of clusters $
				\{c_i\}, i \in 1..K$}
			
			\While{$C'\not= C$}\Comment{While the clusters are not stable}
			
			\State $C \gets C'$
			
			\For{$x \in \mathbf{X}$}
			
			\State cluster(x) $\gets$ \textit{$argmin_{i \in 1..C}$}(dist($c_i, x$))  
			
			\EndFor
			
			\For{$c \in C'$}
			
			\State c $\gets$ average([x $\forall$ cluster(x)  == c])
			\EndFor
			
			\EndWhile\label{euclidendwhile}
			
			\State \textbf{return} $C$\Comment{Return the clusters}
			
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}
	
	\subsection{Gaussian Mixture Models}
	Here we consider more concretely how a hypothetical sample space \textbf{which could be well clustered} might arise:
	
	\subsection{The environment}
	
	(k, x) pairs are generated by performing the following steps:
	
	\begin{itemize}
		\item sample k from ${1..K}$ with $\phi_k = P(K = k) => \sum_{k=1}^{K}\phi_k = 1$
		\item each k corresponds to one of a collections of (possibly multi-variate) Gaussian distributions, with parameters $\mathbf{\mu_k}$ and $\mathbf{\sum_k}$
		\item to select a data point, we sample from the distribution $\mathcal{N}(\mu_k, \sum_k)$
		\item therefore we have a set of data points, however only the oracle knows the labels - referred to as \textbf{latent} variables. We only see the data points. It is our job to infer the possible labels through machine learning
		\item Therefore, our job is to learn both the labels for each data point, and some model parameters to model the underlying distributions of those labels: $\mathbf{\theta := (\phi, \mu_1, \sum_1, ..., \mu_k, \sum_k)}$
		\item note: this seems reminiscent of a hidden Markov model with continuous observed states.
	\end{itemize}
	
	\subsection{Likelihood}
	The probability of a given label and data point pair is: \\
	$P(k, x_n) = P(K=k)P(x_n | K=k) = \phi_k \mathcal{N}(\mathbf{x_n | \mu_k, \sum_k})$ \\ \\
	Since we do not know k, we sum (marginalise) over the unknown: \\
	$P(\mathbf{x}_n) = \sum_{k=1}^{K}\phi_k \mathcal{N}(\mathbf{x_n| \mu_k, \sum_k})$ \\
	
	Log Likelihood of the entire model $\mathbf{\theta}$:
	
	$\mathcal{L}(\theta) = \sum_{n=1}^N\ln p(x_n)$
	
	
	\section{Hard EM for Document Clustering}
	
	\subsection{Notation}
	
	Here we describe the notation used (from Alexandria)
	
	\begin{itemize}	
		\item There are $N$ documents $\{\mathbf{d}_1, ..., \mathbf{d}_N\}$, each having a cluster assignment vector (the latent variable) $\{\mathbf{z}_1, ..., \mathbf{z}_n\}$, which is a sparse matrix containing 1 for the entry corresponding to the cluster, and 0 for all others.
		\item $P(k) = \phi_k$, the probability of a document being of cluster $k$. There are $K$ clusters
		
		\item set of words in a dictionary $w \in \mathcal{A}$
		\item word proportion vectors for each cluster $\mathbf{\mu}_k$, where $\sum_{w \in \mathcal{A}} \mathcal{\mu}_{k,w} = 1$
		\item counts of word $w$ in document $d$: $c(w, d)$
		
		
		\item The mixing components ($\phi_k = \frac{N_k}{N}$) are the total documents with a cluster over the total documents
		
		\item The word proportion parameters in each vector: $$\mu_{k,w} = \frac {sum^N_{n=1}z_{n,k}c(w, d_n)} {\sum{w' \in \mathcal{A}} \sum^N_{n=1} z_{n,k}c(w', d_n)}$$,
		Which is the sum of all words in a cluster over the sum of all words in all documents of the cluster.
		
		\item $\mathbf{\theta} := (\mathbf{\phi}, \mathbf{\mu_1}, ..., \mathbf{\mu_k})$
		
		\item $\gamma(z_{nk}) := p(z_{nk} = 1 | \mathbf{d}_n, \mathbf{\theta})$ is the probability that a document $n$ is of cluster $k$ \\
		
		$$
		P(z_{n,k}) = \frac
		{\phi_k \prod_{w \in \mathcal{A}} \mu^{c(w, d_n)}_{k,w}}
		{\sum^K_{k=1}(\phi_k \prod_{w \in \mathcal{A}} \mu^{c(w, d_n)}_{k,w})} 
		$$
		
	\end{itemize}
	
	\begin{algorithm}
		\caption{Soft EM For Document Clustering}
		\begin{algorithmic}[1]
			\Procedure{SoftEmDoc}{$\mathcal{D} := {d_1, ..., d_n}, K$}\Comment{$\mathcal{D}$ contains $N$ documents, K is the number of clusters}
			
			\State $\mathbf{\theta}^{new} \gets initTheta()$ \Comment{$\mathbf{\theta} := \{\mathbf{\phi}, \mathbf{\mu_1}, ..., \mathbf{\mu_K}\}$}
			
			\Do
			\State $\mathbf{\theta}^{old} \gets \mathbf{\theta}^{new}$
			
			\For{$n \in N$}
			
			\For{$k \in K$}
			
			\State $\gamma(z_{n,k}) = P(z_{n,k} = 1 | \mathbf{d}_n, \mathbf{\theta^{old}})$ \Comment{Expectation (E) Step}
			
			\State $$\mathbf{\phi^{new}} = \frac{\sum^{N}_{n=1}\gamma(z_{n,k})}{N}$$
			
			\State $$\mathbf{\mu_{k,w}} = 
			\frac
			{\sum^{N}_{n=1}\gamma(z_{n,k})c(w, d_n)} {\sum_{w' \in \mathcal{A}} \sum^{N}_{n=1} \gamma(z_{n,k})c(w' d_n)}$$ \Comment{Maximisation (M) Step}
			
			\EndFor
			
			\EndFor
			
			\doWhile{$diff(\mathbf{\theta_{old}}, \mathbf{\theta^{new}}) < \epsilon$}
			
			\State return {$\mathbf{\theta}^{new}$}
			
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{Hard EM For Document Clustering}
		\begin{algorithmic}[1]
			\Procedure{SoftEmDoc}{$\mathcal{D} := {d_1, ..., d_n}, K$}\Comment{$\mathcal{D}$ contains $N$ documents, K is the number of clusters}
			
			\State $\mathbf{\theta}^{new} \gets initTheta()$ \Comment{$\mathbf{\theta} := \{\mathbf{\phi}, \mathbf{\mu_1}, ..., \mathbf{\mu_K}\}$}
			
			\Do
			\State $\mathbf{\theta}^{old} \gets \mathbf{\theta}^{new}$
			
			\For{$n \in N$}
			
			\For{$k \in K$}
			
			\State $\gamma(z_{n,k}) = P(z_{n,k} = 1 | \mathbf{d}_n, \mathbf{\theta^{old}})$ \Comment{Expectation (E) Step}
			
			\State $$\mathbf{\phi^{new}} = \frac{\sum^{N}_{n=1}\gamma(z_{n,k})}{N}$$
			
			\State $$\mathbf{\mu_{k,w}} = 
			\frac
			{\sum^{N}_{n=1}\gamma(z_{n,k})c(w, d_n)} {\sum_{w' \in \mathcal{A}} \sum^{N}_{n=1} \gamma(z_{n,k})c(w' d_n)}$$ \Comment{Maximisation (M) Step}
			
			\EndFor
			
			\EndFor
			
			\doWhile{$diff(\mathbf{\theta_{old}}, \mathbf{\theta^{new}}) < \epsilon$}
			
			\State return {$\mathbf{\theta}^{new}$}
			
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}
	
	\clearpage
	\section{Neural Networks}
    
    \subsection{Formulas to remember}
    
    \begin{itemize}
     \item Sigmoid function: $$ \sigma(z) = \frac{1}{1 + e^{-z}}$$ 
     
     $$\dfrac{\mathrm{d}\sigma}{\mathrm{d}z} = \sigma(z)(1-(\sigma(z)))$$
    
     \item Tanh function: $$ tanh(z) = \frac{e^z-e^{-z}}{e^{z}+e^{-z}} $$
     $$\dfrac{\mathrm{d}tanh(z)}{\mathrm{d}z} = 1-(tanh(z))^2$$
     
     Normal Distribution Formula:
     $$\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2}$$
     
     \item Neuron: $$ H_{W,b} := f(W^Tx) = f(\sum_i W_ix_i + b)$$
    \end{itemize}
    
    \textbf{Textbook Terminology}
    
    \begin{itemize}
        \item nl = number of layers
        \item layer L =  $L_l$ where $L_1$ is the input layer and layer $L_{nl}$ is the output layer
        \item $a_i^l = $ activation after f for neuron i at layer l. $a_i^1 = x_i$ and $a_i^{nl}$  is the final output to be compared to the training data.
        \item $z_i^l = $ weighted sum of inputs to neuron i at layer l.
        \item $W_{ij}^l$ = Weight leading from neuron j in layer l to neuron i in layer l + 1.
        \item $b_i^l$ = Bias term leading towards neuron i in layer l + 1.
    \end{itemize}
    
    \subsection{Activation and Error Functions}
    
    They key for all of the below examples is to understand what probability distribution the target function is modelled under, and therefore how maximising the joint probability could lead to a maximum likelyhood error function.
    
    \paragraph{Regression \\ \\ }
    
    \textbf{Activation Function}: In a regression problem, assuming we aren't making some claim about the range of our true output values, we will need the final output to be able to model any real valued number (or set therefore, but we focus on the single output case here).
    Therefore, while the hidden layers may have non-linear activation functions, the final layer must be linear (e.g. just the sum of previous layers. \\ 
    
    \textbf{Distribution}: \\
    
    Assume a Gaussian model: $$P(y | x) = \prod_{n \in 1..N} \mathcal{N}(x_nW, \sigma)$$
    
    After taking the negative log likelyhood and doing some math, we get the sum of squared errors: \\
    
    \textbf{Error Function}: \\
    $$ J(\theta) = \frac{1}{2} \sum_{n \in N} (X_nW + b - y)^2$$
    
    
    \paragraph{Single Class Classification\\ \\ }
    
    \textbf{Activation Function}
    
    We assume need to output a probability of being in the singleton class, so the \textbf{sigmoid} function will work. \\
    
    \textbf{Distribution}: \\
    We must assume a binomial distribution, with probability $\prod_{n \in N} h_n^{y_n} (1 - h_n^{y_n})^{1-y_n}$ \\
    
    \textbf{Error Function} \\
    When we take the negative log of this, we end up getting \textbf{Cross Entropy Loss}: 
    
    $$J(\sigma) = -\sum_{n \in N} y^n log(h_n) - (1-y_n)log(1-h_n)$$
    
   
    \paragraph{Multi Class Classification\\ \\ }
    
    \textbf{Activation Function}
    
    We will use \textbf{softmax} activation function: softmax transforms any real valued vector into a probability 
    distribution with the following formula: \\
    
    For each element i of vector v: $$softmax(v_i) = \frac{e^{v_i}}{\sum_{v_n \in v}e^{v_n}}$$
    
    \textbf{Distribution}: \\
    We also binomial distribution, with probability $\prod_{n \in N} \prod_{k \in Classes}  h_n^{y_{nk}} (1 - h_n^{y_{nk}})^{1-y_{nk}}$ \\
    
    \textbf{Error Function} \\
    When we take the negative log of this, we end up getting \textbf{Cross Entropy Loss}: 
    
    $$J(\sigma) = -\sum_{n \in N} \sum_{k \in Classes} y^{nk} log(h_{n}) - (1-y_{nk})log(1-h_n)$$
   
    \subsection{Back Propogation}
   
    Now comes the final piece of the puzzle
    The question is, how do we calculate the partial derivatives for the weights? Back propogation is able to give us $\dfrac{d}{d W_{ij}^{l}}J(\theta, x, y)$ for every possible weight by re-using the activations calculated in the forward pass. \\ 
    
    First we recall the gradient descent algorithm:
    
    \begin{enumerate}
	
	\item Initialize random weights $\pmb{w}$, set regularization parameter $\lambda$, stopping condition $\epsilon$, and training rate $\eta$
	\item While the difference between weights from the previous step is less than $\epsilon$:
	
	\setlength{\itemindent}{2em}
	\item Randomly shuffle the training data rows
	\item For each training example input and output pair $(\pmb{x}_n, y_n)$:
	
	\setlength{\itemindent}{4em}
	\item Calculate the gradient vector 
	$\frac{d}{d\pmb{w}}J(\theta)$ for each weight \newline 
	
	\item Perform weight update: $\pmb{w} = \pmb{w} - \eta \frac{d\pmb{e}_{L2}}{d\pmb{w}}$
   
	\end{enumerate}
        
    Now comes the calculation of those above gradients with back-propogation: 
    
    \begin{enumerate}
     \item Perform a feedforward pass, computing actuvations for layers $L2,...,L_nl$
     \item For each output unit i in layer nl, set $\delta^{nl} = -(y_i - a_i^{nl})f'(z_i^{nl})$ 
     \item for all the subsequent layers, and for each node i in layer l: set $\delta_i^l = (\sum_{j=1}^{s_l+1}W_{ji}^{l}\delta_j^{l+1})f'(z_i^l)$
     \item $\frac{d}{dW_{ij}^l}J(\theta, x, y) = a_j^l\delta_i^{l+1}$
    \end{enumerate}

    \subsection{Regularisation}
    There are a few different techniques for avoiding over fitting. Here they are:
    
    Weight Decay: adding a component to the error term to account for the magnitude of weights. \\
    
    Early Stopping: Stop with respect the smallest error of the validation set. \\
    
    Both of these techniques (in the case of a quadratic regularisation term) have a similar effect in limiting the effective complexity of the model.
    
    \subsection{Autoencoders}
    
    Mapping inputs onto themselves. Autoencoders are effectively performing principal component analysis. In the case of linear activation functions, the error function has a unique global minimum.  
    
    Having a non linear activation function in the single middle hidden layer will not cause the autoencoder to perform non linear principal component analysis - instead, you need extra hidden layers surrounding the middle layer with non linear activations.
    
	\clearpage
	\section{Big Data}
	
	\subsection{Overview}
	
	\textit{Big} data is useful for training models which can grow in complexity as the data size increases - for example, deep neural networks. Not all models will grow with the training size though - naive bayes is good on small training sets, but quickly reaches it's upper bound of performance as data is added. \\
	
	The challenge though is training these models. Parallelisation can occur when we have multiple CPUs or computers (network latency being an issue in the later case). \\
	
	Where algorithms contain sums over the data set, we can split up the computation as portions of the sum and compute each portion on a different machine. \\
	
	\subsection{Batch Gradient Descent}
	
	\textit{In English}: in batch gradient descent, the gradient update is the sum of all individual weight updates (generally the partial derivatives of the zeroed error function). We can calculate that by giving the current model to each computer, but only subsets of the data to each computer. Each computer calculates its own error, then the master node sums them up to get the total weight update. All the models can then we updated. 
	
	Question: how to do this with stochastic gradient descent? Apparently it's some asynchronous update.
	
	\begin{algorithm}
		\caption{Map Reduce for Gradient Descent}
			\begin{algorithmic}[5]
				\Procedure{Distributed Gradient Descent}{$\mathcal{D} := {d_1, ..., d_m}$}  \Comment{$\mathcal{D}$ is divided into M subsection}
				
				\State $\mathbf{w} \gets init()$ \Comment{parameter vector}
				
				\While{stopping condition not met}
					
					\State doInParallel($temp_m \gets \delta \mathcal{E}_m(\mathbf{w})$)
					
					\State $\mathbf{w} \gets \mathbf{w} + \sum_{m=1}^M temp_m$
				
				\EndWhile
				
				\EndProcedure
			\end{algorithmic}
		\end{algorithm}
		
	
	\subsection{Distributed K-Means}
	
	\textit{In English}: here we calculate clusters for each subset of the data (as the average of the previously assigned points to that cluster) as well as the number of points assigned for that subset, then using all those averages (k * m, given k clusters and m machines) we can take a weighted average to get what would have been the overall average. The key here is that any correctly weighted average of a bunch of averages is the same as the average of all points :)
	
	\begin{algorithm}
		\caption{Map Reduce for Gradient Descent}
		\begin{algorithmic}[6]
			\Procedure{Distributed Gradient Descent}{$\mathcal{D} := {d_1, ..., d_m}$}  \Comment{$\mathcal{D}$ is divided into M subsection}
			
			\State $\mathbf{\mu_k} \gets initRandom(), \forall k$ \Comment{init mu}
			
			\While{stopping condition not met}
			
			\State $\mathbf{\mu_k'} \gets 0, \forall k$ \Comment{init mu}
			\State $\mathbf{n_k} \gets 0, \forall k$ \Comment{init n}
			
			\Comment{Map}
			5
			\State $doInParallel_k$($\mathbf{\mu_{km}} \gets calcCluster(pts=D_m,cluster=k)$)
			\State $doInParallel_k$($\mathbf{n_{km}} \gets pointsInCluster(pts=D_m,cluster=k)$)
			
			\Comment{Reduce}
			
			\State $$\mu_k \gets \frac{\sum_{m=1}^{M} \mu_{km}}{\sum_{m=1}^{M}n_{km}}$$
			
			\EndWhile
			
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}
	
	\subsection{Distributed Expectation Maximisation for Gaussian Mixture Models}
	
	Similar to the above examples, but a little more complicated.
	
	
	% bibliography
	\bibliographystyle{ieeetran}
	\bibliography{ml_references}
	
	
\end{document}
